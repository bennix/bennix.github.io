
---
layout: post
title: 使用PyTorch深入学习入门！
date: 2018-07-29
categories: blog
tags: [PyTorch,入门]
description: 使用PyTorch深入学习入门！
---

# 第一章 使用PyTorch深入学习入门！

深度学习（DL）已经彻底改变了工业界。 Andrew Ng 在Twitter上曾经有一次着名的描述：
人工智能是新的电力！

电力转变了无数的行业; 人工智能（AI）现在也会这样做。

AI和DL像同义词一样使用，但两者之间存在很大差异。 让我们揭开行业中使用的术语的神秘面纱，这样你作为一名从业者，就能够区分信号和噪音。

在本章中，我们将介绍AI的以下不同部分：

* AI本身及其起源
* 现实世界中的机器学习
* 深度学习的应用
* 为什么要深入学习？
* 深度学习框架：PyTorch

## 人工智能
每天都会发表无数文章讨论人工智能。这一趋势在过去两年中有所增加。人工智能在网络上有几种定义，我最喜欢的是通常由人类执行的智能任务的自动化。

## AI的历史
人工智能这个术语最早是由John McCarthy于1956年创造的，当时他举办了第一次关于这一主题的学术会议。关于机器是否思考的问题的旅程要早得多。在人工智能的早期，机器能够解决人类难以解决的问题。

例如，Enigma机器是在第二次世界大战结束时建造的，用于军事通信。 Alan Turing构建了一个帮助破解Enigma代码的AI系统。破解Enigma代码对于人来说是一项非常具有挑战性的任务，分析师可能需要数周才能完成。 AI机器能够在几小时内破解代码。

计算机很难解决我们直观的问题，例如区分狗和猫，告诉你的朋友是否因为迟到派对而感到生气（情绪），区分卡车和汽车，在记录期间记笔记研讨会（语音识别），或将笔记转换为另一种语言，你是你的朋友，不懂你的语言（例如，法语到英语）。这些任务中的大多数对我们来说都是直观的，但我们无法对计算机进行编程或硬编码以执行这些任务。早期AI机器中的大部分智能都是硬编码的，例如下棋的计算机程序。

在AI的早期阶段，许多研究人员认为AI可以通过硬编码规则来实现。这种AI被称为符号AI，可用于解决定义明确的逻辑问题，但它几乎无法解决复杂的问题，如图像识别，对象检测，对象分割，语言翻译和自然语言理解任务。 。为了解决这些问题，开发了更新的AI方法，例如机器学习和DL。

为了更好地理解AI，ML和DL之间的关系，让我们将它们视为具有AI的同心圆 - 首先出现的想法（最大的），然后是机器学习 - （后来开始发展），最后是DL-正在推动今天的AI爆炸（适合两者）：

![](https://bennix.github.io/imgs/1_1.png)

## 机器学习
机器学习（ML）是人工智能的一个子领域，并且在过去10年中变得流行，有时两者可互换使用。 除了机器学习之外，AI还有很多其他子领域。 ML系统是通过显示大量示例而构建的，与符号AI不同，我们通过硬编码规则来构建系统。 从高层次来看，机器学习系统会查看大量数据并提出预测未见数据结果的规则：

![](https://bennix.github.io/imgs/1_2.png)

大多数ML算法在结构化数据上表现良好，例如销售预测，推荐系统和营销个性化。任何ML算法的一个重要因素是特征工程，数据科学家需要花费大量时间来使ML算法能够正常运行。在某些领域，例如计算机视觉和自然语言处理（NLP），特征工程具有挑战性，因为它们具有高维度。

直到最近，出于诸如特征工程和高维度等原因，这样的问题对于组织使用典型的机器学习技术（例如线性回归，随机森林等）来解决具有挑战性。考虑尺寸为224 x 224 x 3（高x宽x通道）的图像，其中图像尺寸中的3表示彩色图像中红色，绿色和蓝色通道的值。要将此图像存储在计算机内存中，我们的矩阵将包含150,528个单个图像的尺寸。假设您要在尺寸为224 x 224 x 3的1,000张图像上构建分类器，尺寸将变为150,528的1,000倍。机器学习的一个特殊分支称为深度学习，它允许您使用现代技术和硬件来处理这些问题。


## 现实生活中机器学习的例子
以下是一些由机器学习提供支持的酷炫产品：
* 示例1：Google相册使用称为深度学习的特定形式的机器学习来对照片进行分组
* 示例2：推荐系统是一系列ML算法，用于推荐Netflix，亚马逊和iTunes等主要公司的电影，音乐和产品

## 深度学习

传统的ML算法使用手写特征提取来训练算法，而DL算法使用现代技术以自动方式提取这些特征。

例如，预测图像是否包含面部的DL算法提取诸如第一层检测边缘的特征，第二层检测诸如鼻子和眼睛的形状，以及最终层检测面部形状或更复杂的结构。 每个层基于先前层的数据表示来训练。 如果你发现这个解释很难理解，没有关系，本书后面的章节将帮助你直观地构建和检查这样的网络：

![](https://bennix.github.io/imgs/1_3.png)

随着GPU，大数据，Amazon Web Services（AWS）和Google Cloud等云提供商以及Torch，TensorFlow，Caffe和PyTorch等框架的兴起，DL的使用在过去几年中得到了极大的发展。 除此之外，大型公司共享在大型数据集上训练的算法，从而帮助初创公司轻松地在几个用例上构建最先进的系统。

## 深度学习的应用

使用DL实现的一些流行应用程序如下：
* 近人类级别的图像分类
* 近人类语音识别
* 机器翻译
* 自动驾驶汽车
* Siri，Google Voice和Alexa近年来变得更加准确
* 日本农民分拣黄瓜
* 肺癌检测
* 语言翻译超越了人类的准确性

以下屏幕截图显示了摘要的简短示例，其中计算机采用大段文本并将其汇总为几行：

![](https://bennix.github.io/imgs/1_4.png)

在下面的图像中，计算机被给予了一个普通的图像而没有被告知它显示的内容，并且使用对象检测和字典的一些帮助，你得到一个图像标题，说明两个年轻女孩正在玩乐高玩具。 不是很棒吗？

![](https://bennix.github.io/imgs/1_5.png)

## 与深度学习相关的炒作

媒体中的人和人工智能领域以外的人，或者不是AI和DL真正实践者的人，一直在暗示像电影“终结者2：审判日”这样的故事情节可能会随着AI / DL的发展而成为现实。他们中的一些人甚至谈论我们将被机器人控制的时间，机器人决定什么对人类有益。目前，AI的能力被夸大了，远远超出了它的真实能力。目前，大多数DL系统部署在非常受控的环境中，并且给出了有限的决策边界。

我的猜测是，当这些系统可以学习做出明智的决策时，而不仅仅是完成模式匹配，当数百或数千个DL算法可以协同工作时，那么我们可能会看到机器人可能表现得像我们看到的那样在科幻电影中。实际上，我们并不接近一般的人工智能，机器可以做任何事情而不被告知这样做。 DL的当前状态更多是从现有数据中寻找模式以预测未来结果。作为DL从业者，我们需要区分信号和噪声。


## 深度学习的历史

尽管深度学习近年来已经变得流行，但深度学习背后的理论自20世纪50年代以来一直在发展。 下表显示了目前在DL应用程序中使用的一些最流行的技术及其大致时间表：

|技术|年|
|---|---|
|神经网络|1943|
|反向传播|20世纪60年代初|
|卷积神经网络|1979|
|递归神经网络|1980|
|长期短期记忆|1997|

多年来，深度学习已经被赋予了几个名字。 它在20世纪70年代被称为控制论，在20世纪80年代被称为连接主义，现在它被称为深度学习或神经网络。 我们将交替使用DL和神经网络。 神经网络通常被称为受人类大脑工作启发的算法。 然而，作为DL的实践者，我们需要理解它主要受到数学（线性代数和微积分），统计学（概率）和软件工程的强大理论的启发和支持。

## 为什么现在？
为什么DL现在变得如此受欢迎？ 一些重要原因如下：

* 硬件可用性
* 数据和算法
* 深度学习框架

## 硬件可用性
深度学习需要对数百万（有时是数十亿）参数执行复杂的数学运算。现有的CPU需要很长时间才能执行这些操作，尽管这在过去几年中有所改进。一种称为图形处理单元（GPU）的新型硬件已经完成了这些巨大的数学运算，例如矩阵乘法，数量级更快。

GPU最初是由Nvidia和AMD等公司为游戏行业构建的。事实证明，这种硬件非常高效，不仅可以渲染高质量的视频游戏，还可以加速DL算法。来自Nvidia的最新GPU，即1080ti，需要几天时间才能在imageNet数据集之上构建图像分类系统，以前可能需要一个月左右的时间。

如果您打算购买用于运行深度学习的硬件，我建议您根据预算从Nvidia中选择GPU。选择具有大量内存的一个。请记住，您的计算机内存和GPU内存是两回事。 1080ti配备11 GB内存，售价约700美元。

您还可以使用各种云提供商，例如AWS，Google Cloud或Floyd（该公司提供针对DL优化的GPU机器）。如果您刚刚开始使用DL，或者如果您正在为可能拥有更多财务自由的组织使用设置计算机，则使用云提供商是经济的。

>如果优化这些系统，性能可能会有所不同

下图显示了一些比较CPU和GPU之间性能的基准测试：

![](https://bennix.github.io/imgs/1_6.png)

## 数据和算法

数据是深度学习成功的最重要因素。由于互联网的广泛采用和智能手机的日益普及，Facebook和谷歌等几家公司已经能够以各种格式收集大量数据，特别是文本，图像，视频和音频。在计算机视觉领域，ImageNet竞赛在为1,000个类别提供140万张图像的数据集方面发挥了巨大作用。

这些类别是手工注释的，每年都有数百个团队参与竞争。在竞争中取得成功的一些算法是VGG，ResNet，Inception，DenseNet等等。这些算法现在用于工业中以解决各种计算机视觉问题。在深度学习空间中经常用于对各种算法进行基准测试的一些其他流行数据集如下：

* MNIST
* COCO数据集
* CIFAR
* 街景房号
* PASCAL VOC
* 维基百科转储
* 20个新闻组
* 宾州树银行
* Kaggle

不同算法的增长，例如批量标准化，激活功能，跳过连接，长期短期记忆（LSTM），Dropout 等等，使得近年来能够更快，更成功地训练非常深的网络成为可能。在本书的后续章节中，我们将详细介绍每种技术以及它们如何帮助构建更好的模型。

## 深度学习框架

在早期，人们需要具备C ++和CUDA的专业知识来实现DL算法。 现在许多组织都在开源他们的深度学习框架，具有脚本语言知识的人（如Python）可以开始构建和使用DL算法。 目前业内使用的一些流行的深度学习框架是TensorFlow，Caffe2，Keras，Theano，PyTorch，Chainer，DyNet，MXNet和CNTK。

如果没有这些框架，深度学习的采用就不会那么大。 它们消除了许多潜在的复杂性，使我们能够专注于应用程序。 我们仍然处于DL的早期阶段，通过大量的研究，每天都在公司和组织中发生突破。 因此，各种框架各有利弊。

## PyTorch

PyTorch和大多数其他深度学习框架可用于两个不同的事情：
* 使用GPU加速操作替换类似NumPy的操作
* 构建深度神经网络

让PyTorch越来越受欢迎的是它的易用性和简单性。与使用静态计算图的大多数其他流行的深度学习框架不同，PyTorch使用动态计算，这允许在构建复杂体系结构时具有更大的灵活性。

PyTorch广泛使用Python概念，例如类，结构和条件循环，允许我们以纯面向对象的方式构建DL算法。大多数其他流行的框架带来了他们自己的编程风格，有时使编写新算法变得复杂，并且它不支持直观的调试。在后面的章节中，我们将详细讨论计算图。

尽管PyTorch最近发布并且仍然处于测试版本，但它在数据科学家和深度学习研究人员中因其易用性，更好的性能，更易于调试的性质以及各种公司（如各种公司）的强大支持而受到极大的欢迎。销售队伍。

由于PyTorch主要用于研究，因此在延迟要求非常高的某些情况下不建议用于生产。然而，这一变化正在改变一个名为Open Neural Network Exchange（ONNX）的新项目（https://onnx.ai/ ），该项目的重点是将在PyTorch上开发的模型部署到像生产就绪的Caffe2这样的平台上。在撰写本文时，现在就对这个项目说太多还为时过早，因为它刚刚启动。该项目得到了Facebook和微软的支持。

在本书的其余部分，我们将了解用于在计算机视觉和NLP领域构建强大的DL应用程序的各种乐高模块（较小的概念或技术）。

## 小结

在这个介绍性章节中，我们探讨了人工智能，机器学习和深度学习是什么，我们讨论了三者之间的差异。 我们还在日常生活中查看了由他们提供支持的应用程序。 我们深入研究为什么DL现在变得越来越受欢迎。 最后，我们对PyTorch进行了温和的介绍，这是一个深度学习框架。

在下一章中，我们将在PyTorch中训练我们的第一个神经网络。
